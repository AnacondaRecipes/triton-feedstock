{% set version = "3.1.0" %}
# Triton no longer tags releases, but there are release branches, e.g.
# https://github.com/triton-lang/triton/commits/release/3.1.x/
# Check if the commit id from Pytorch's latest pinned commit in
# https://github.com/pytorch/pytorch/blob/v{{ pytorch_ver }}/.ci/docker/ci_commit_pins/triton.txt
# can be found on one of those release branches, and use that as the version
{% set git_commit = "5fe38ffd73c2ac6ed6323b554205186696631c6f" %}

package:
  name: triton
  version: {{ version }}

source:
  url: https://github.com/openai/triton/archive/{{ git_commit }}.tar.gz
  sha256: 933babc32b69872efbce05fe8be61129fecf52c724fadea42d8c7b2d10e16ad9
  patches:
    - patches/0001-Remove-Werror-that-cause-false-positive-build-failur.patch
    # https://github.com/triton-lang/triton/commit/e4569136f3821ad3d99bef43254bf935c1c96f42
    - patches/0002-BACKEND-Update-LLVM-version-to-https-github.com-llvm.patch
    # https://github.com/triton-lang/triton/commit/cd4a172c79c69fff91b893c2e5deb78a7a887c26
    - patches/0003-BACKEND-Update-LLVM-version-to-https-github.com-llvm.patch
    # https://github.com/triton-lang/triton/commit/3e233d7ccd62bc7a29eb5266c80b379cef1f6132
    - patches/0004-BACKEND-Update-LLVM-to-llvm-llvm-project-657ec7320d8.patch
    # https://github.com/triton-lang/triton/commit/e8873ae7dfe68eda04d7656ec93627afe8dc56a1
    - patches/0005-BACKEND-Update-LLVM-version-to-https-github.com-llvm.patch
    # https://github.com/triton-lang/triton/commit/46550ab18e8f7314107cf591b9cf902b290fd45d
    - patches/0006-BACKEND-Update-LLVM-version-to-https-github.com-llvm.patch
    # diff between main branch:
    # https://github.com/triton-lang/triton/commit/cf2ad02324fc253970c3ab2666e775406405f213
    # and 3.1.x branch:
    # https://github.com/triton-lang/triton/commit/757b6a61e7df814ba806f498f8bb3160f84b120c
    - patches/0007-Update-config.enableRegionSimplification-for-LLVM-19.patch
    - patches/0008-Do-not-link-directly-to-LLVM-static-libraries.patch
    # https://github.com/triton-lang/triton/commit/f48dbc1b106c93144c198fbf3c4f30b2aab9d242
    - patches/0009-CODEGEN-Support-CUDA-12.6-4588.patch
    - patches/0010-Use-system-PATH-to-find-tools-in-CONDA_PREFIX.patch
    # backport https://github.com/triton-lang/triton/pull/5492
    - patches/0011-Add-conda-forge-include-dirs-to-list-of-include-dirs.patch

build:
  number: 3 
  # TODO: windows support should be available from next version;
  #       CPU-only support still under development
  skip: true  # [win or cuda_compiler_version == "None"]
  # We only use this (and can test this) for pytorch, linux-64/GPU.
  skip: true  # [not (linux and x86_64)]
  string: cuda{{ cuda_compiler_version | replace('.', '') }}py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}

requirements:
  build:
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}
    - ninja
    - cmake
    - mlir
    - llvmdev
    - patch
  host:
    - python
    - pybind11
    - pip
    - setuptools
    - wheel
    - llvm
    - libmlir
    - zlib
    - nlohmann_json
    - cuda-cupti-dev
  run:
    - python
    - filelock
    - lit
    - cuda-nvcc
    - cuda-cuobjdump
    - cuda-cudart
    - cuda-cupti

# Note that PyTorch is a test dependency here, and Triton is a dependency of (the CUDA variant of) PyTorch.
# So, you need to build Triton without running the tests (`conda build --no-test`), then build PyTorch, then run these tests.
test:
  imports:
    - triton
    - triton._C.libtriton
  requires:
    - pip
    - pytest
    - scipy
    - pytorch=*=*cuda*
  source_files:
    - python/test
  commands:
    - pip check
    # test suite essentially depends on availability of a physical GPU,
    # see https://github.com/openai/triton/issues/466;
    # run a test that does not require a GPU but checks
    # if triton.compile() works
    - pytest -v python/test/unit/tools/test_aot.py::test_ttgir_to_ptx
    # Here is a list of current test failures and reasoning why they're ok:
    #
    # test_dummy_backend                    - looks like it's using CUDA instead of CPU backend for this test, for some reason. We don't need to use the CPU backend anyway.
    # IndexError: map::at errors            - known issue for T4 GPUs https://github.com/triton-lang/triton/issues/3787
    # out of resource: shared memory errors - fine, just platform resource is less than expected
    # test_print[device_print_large-int32]  - assert False - looks like a print output error, works fine for other data types, should be ok
    # test_compile_in_forked_subproc        - AssertionError: assert 1 == 0 - also an IndexError: map::at output (shown in the stderr output)
    #
    # In general, the more important tests are the PyTorch tests. This package only supports PyTorch. See text at the top of the recipe.
    #
    # the test_performance tests are broken for compute capability 7.x, which applies to our current build instances.
    - pytest -v python/test --ignore=python/test/regression/test_performance.py || true

about:
  home: https://github.com/openai/triton
  license: MIT
  license_family: MIT
  license_file: LICENSE
  summary: Development repository for the Triton language and compiler
  description: |
    This is the development repository of Triton, a language and compiler for writing highly efficient custom Deep-Learning primitives.
    The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs.
  doc_url: https://triton-lang.org/
  dev_url: https://github.com/openai/triton

extra:
  recipe-maintainers:
    - erip
    - h-vetinari
